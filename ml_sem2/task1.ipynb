{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ce97b0-6933-4f37-a268-23a4586df658",
   "metadata": {},
   "source": [
    "Задача 1. Распознавания именованных сущностей на основе fasttext\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a68a23-3668-419b-9759-f0eda5e2f81f",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "[Crawl vectors](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "\\\n",
    "[How to load model](https://fasttext.cc/docs/en/crawl-vectors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074e52a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ee2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lime nerus fasttext # necessary libs in case not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69e553a-0ac7-4f76-a3fa-5eaeebea234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 18:54:49.046552: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-05-10 18:54:49.046592: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# fasttext (!)\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# necessary utils \n",
    "from nerus import load_nerus\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "# for progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ml\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf79d481-f812-41c9-998e-2ac65c9c43e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dfe66",
   "metadata": {},
   "source": [
    "### Loading NERUS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fa0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "nerus_db_location = Path('./').resolve() / 'nerus_lenta.conllu.gz' # I assume it's in the root of working directory\n",
    "assert nerus_db_location.exists() # check if file exists\n",
    "nerus_db = load_nerus(nerus_db_location) # loading db as in example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fa9333-7918-4cdf-857f-9e399f1c6b80",
   "metadata": {},
   "source": [
    "### Token structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06af9dfb-685f-4259-bbfc-759e184f3ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NerusToken(\n",
       "    id='3',\n",
       "    text='социальным',\n",
       "    pos='ADJ',\n",
       "    feats={'Case': 'Dat',\n",
       "     'Degree': 'Pos',\n",
       "     'Number': 'Plur'},\n",
       "    head_id='4',\n",
       "    rel='amod',\n",
       "    tag='O'\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_text = next(nerus_db)\n",
    "\n",
    "token = next_text.sents[int(np.random.randint(6))].tokens[int(np.random.randint(6))]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d34533",
   "metadata": {},
   "source": [
    "### Getting 5000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7f70a1-6d8a-48ad-8f60-d83b684e907e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e885e774855d496ebc8e666d2f6d2d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Австрийские', 'правоохранительные', 'органы', 'не', 'представили', 'доказательств', 'нарушения', 'российскими', 'биатлонистами', 'антидопинговых', 'правил', '.')\n",
      "('Об', 'этом', 'сообщил', 'посол', 'России', 'в', 'Вене', 'Дмитрий', 'Любинский', 'по', 'итогам', 'встречи', 'уполномоченного', 'адвоката', 'дипмиссии', 'с', 'представителями', 'прокуратуры', 'страны', ',', 'передает', 'ТАСС', '.')\n",
      "('«', 'Действует', 'презумпция', 'невиновности', '.')\n",
      "('Каких-либо', 'ограничений', 'свободы', 'передвижения', 'для', 'команды', 'нет', '»', ',', '—', 'добавили', 'в', 'посольстве', '.')\n",
      "('Международный', 'союз', 'биатлонистов', '(', 'IBU', ')', 'также', 'не', 'будет', 'применять', 'санкции', 'к', 'российским', 'биатлонистам', '.')\n",
      "('Все', 'они', 'продолжат', 'выступление', 'на', 'Кубке', 'мира', '.')\n",
      "('Полиция', 'нагрянула', 'в', 'отель', 'сборной', 'России', 'в', 'Хохфильцене', 'вечером', '12', 'декабря', '.')\n",
      "('Как', 'написал', 'биатлонист', 'Александр', 'Логинов', ',', 'их', 'считают', 'виновными', 'в', 'махинациях', 'с', 'переливанием', 'крови', '.')\n"
     ]
    }
   ],
   "source": [
    "TOKEN_LIST_SIZE = 5000\n",
    "\n",
    "sents = []\n",
    "tags = []\n",
    "for _ in tqdm(range(5000)):\n",
    "    next_test = next(nerus_db)\n",
    "    \n",
    "    for sent in next_test.sents:\n",
    "        sent_toks,sent_tags = zip(*[(tok.text, tok.pos) for tok in sent.tokens])\n",
    "        sents.append(sent_toks)\n",
    "        tags.append(sent_tags)\n",
    "        \n",
    "print(*sents[:8], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018ae88",
   "metadata": {},
   "source": [
    "### Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc399440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def list_columns(obj, cols=4, columnwise=True, gap=4):\n",
    "    sobj = [str(item) for item in obj]\n",
    "    if cols > len(sobj): cols = len(sobj)\n",
    "    max_len = max([len(item) for item in sobj])\n",
    "    if columnwise: cols = int(math.ceil(float(len(sobj)) / float(cols)))\n",
    "    plist = [sobj[i: i+cols] for i in range(0, len(sobj), cols)]\n",
    "    if columnwise:\n",
    "        if not len(plist[-1]) == cols:\n",
    "            plist[-1].extend(['']*(len(sobj) - len(plist[-1])))\n",
    "        plist = zip(*plist)\n",
    "    printer = '\\n'.join([\n",
    "        ''.join([c.ljust(max_len + gap) for c in p])\n",
    "        for p in plist])\n",
    "    print(printer)\n",
    "    \n",
    "def list_dict(freqs):\t\n",
    "\tfor k, v in freqs.items():\n",
    "\t\tprint(f'{k:<4} {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011946ba",
   "metadata": {},
   "source": [
    "### Splitting to train / test and counting unique tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc03a83-2df6-4374-859a-15b0b8ceab21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', '[PAD]']\n",
      "\n",
      "\n",
      "\n",
      "ADP        PRON       ADJ        DET        \n",
      "NOUN       NUM        PUNCT      PART       \n",
      "SYM        AUX        INTJ       X          \n",
      "[PAD]      VERB       ADV                   \n",
      "PROPN      SCONJ      CCONJ                 \n"
     ]
    }
   ],
   "source": [
    "numpyarr_sents = np.array(list(zip_longest(*sents, fillvalue=\"\"))).T\n",
    "numpyarr_tags  = np.array(list(zip_longest(*tags, fillvalue=\"[PAD]\"))).T\n",
    "\n",
    "mask = np.random.rand(len(sents)) < 0.7\n",
    "\n",
    "# as in examples\n",
    "X_train = numpyarr_sents[mask]\n",
    "y_train = numpyarr_tags[mask]\n",
    "X_test = numpyarr_sents[~mask]\n",
    "y_test = numpyarr_tags[~mask]\n",
    "\n",
    "# making train and test\n",
    "d_train = [X_train, y_train]\n",
    "d_test  = [X_test, y_test]\n",
    "\n",
    "# make unique tags\n",
    "unique_tags = list(sorted(set([''] + [target_tag for setn in y_train for target_tag in setn])))\n",
    "tags_in_y_train = [target_tag for y in y_train for target_tag in y]\n",
    "\n",
    "# count all\n",
    "tags_cnt = dict()\n",
    "for target_tag in set(tags_in_y_train):\n",
    "    tags_cnt[target_tag] = tags_in_y_train.count(target_tag)\n",
    "\n",
    "# shortened arrays\n",
    "print(unique_tags)\n",
    "print(\"\\n\\n\")\n",
    "list_columns(tags_cnt, gap=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992803cc",
   "metadata": {},
   "source": [
    "### Loading fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d299e1ee-187f-4027-97de-54317c0991ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# fasttext.util.download_model('ru', if_exists='ignore')\n",
    "original_model = fasttext.load_model('cc.ru.300.bin')\n",
    "# saving space\n",
    "reduced_model = fasttext.util.reduce_model(original_model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91f92cb-e24b-4891-9d52-833131dac637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d227c13ecdb848a1800670b82d2c359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "ADJ  1\n",
      "ADP  2\n",
      "ADV  3\n",
      "AUX  4\n",
      "CCONJ 5\n",
      "DET  6\n",
      "INTJ 7\n",
      "NOUN 8\n",
      "NUM  9\n",
      "PART 10\n",
      "PRON 11\n",
      "PROPN 12\n",
      "PUNCT 13\n",
      "SCONJ 14\n",
      "SYM  15\n",
      "VERB 16\n",
      "X    17\n",
      "[PAD] 18\n",
      "\n",
      "\n",
      "\n",
      "0    \n",
      "1    ADJ\n",
      "2    ADP\n",
      "3    ADV\n",
      "4    AUX\n",
      "5    CCONJ\n",
      "6    DET\n",
      "7    INTJ\n",
      "8    NOUN\n",
      "9    NUM\n",
      "10   PART\n",
      "11   PRON\n",
      "12   PROPN\n",
      "13   PUNCT\n",
      "14   SCONJ\n",
      "15   SYM\n",
      "16   VERB\n",
      "17   X\n",
      "18   [PAD]\n"
     ]
    }
   ],
   "source": [
    "word_to_index, id_to_word = dict(), dict()\n",
    "\n",
    "fst_arr = list()\n",
    "for idx, word in enumerate(tqdm(reduced_model.get_words(on_unicode_error='replace'))):\n",
    "    word_vector = reduced_model.get_word_vector(word)\n",
    "    # check  if we didnt add word yet\n",
    "    if word not in word_to_index:\n",
    "        fst_arr.append(word_vector)\n",
    "        word_to_index[word], id_to_word[idx] = idx, word\n",
    "\n",
    "# add [PAD] fields\n",
    "word_to_index['[PAD]']   = len(word_to_index)\n",
    "word_to_index['[UNK]'] = len(word_to_index)\n",
    "\n",
    "for _ in range(2):\n",
    "\tfst_arr.append(np.zeros_like(fst_arr[-1]))\n",
    "\n",
    "# add ids\n",
    "id_to_word[len(id_to_word)] = '[UNK]'\n",
    "id_to_word[len(id_to_word)] = '[PAD]'\n",
    "\n",
    "# making tags/id and id/tags dicts\n",
    "tag_to_id = {tag : i for (i, tag) in enumerate(unique_tags)}\n",
    "id_to_tag = {i : tag for (i, tag) in enumerate(unique_tags)}\n",
    "\n",
    "#print to check\n",
    "list_dict(tag_to_id)\n",
    "print(\"\\n\\n\")\n",
    "list_dict(id_to_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b299f19-6a9e-4cec-abbc-e03afcdeda83",
   "metadata": {},
   "source": [
    "## Probing model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f42d61-4776-4b19-80e4-5a70b8c5a03c",
   "metadata": {},
   "source": [
    "## Make dataloader\n",
    "\n",
    "Make a NERUSStorage class suitable for easy work with dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08d3988-7ae7-4723-99a7-1a9aef1b32f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58ec6f5b5be4da494b5ea87fbdbce2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NERUSstorage(Dataset):\n",
    "    def __init__(self, dataset, token_to_index, tag_to_id):\n",
    "        self.x_data, self.y_data, self.t2i, self.tg2i  = dataset[0], dataset[1], token_to_index, tag_to_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "# make dataset into array of NERUSstorages\n",
    "total_len = len(d_train[0][0])\n",
    "\n",
    "x_train_u, y_train_u = list(), list()\n",
    "for idx in tqdm(range(len(d_train[0]))):\n",
    "    x_train_u.append(torch.LongTensor([word_to_index.get(tag, word_to_index['[UNK]']) for tag in d_train[0][idx]]))\n",
    "    y_train_u.append(torch.LongTensor([tag_to_id.get(tag, word_to_index['X']) for tag in d_train[1][idx]]))\n",
    "    \n",
    "# getting the dataset\n",
    "dataset = NERUSstorage([x_train_u, y_train_u], word_to_index, tag_to_id)\n",
    "test_dataset = NERUSstorage([x_train_u, y_train_u], word_to_index, tag_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aead67a-719e-4e63-aaf2-a0b56b8af931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting dataloader with batches of size 64\n",
    "dl_train = DataLoader(dataset,      batch_size=64, shuffle=True)\n",
    "dl_test  = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d7f09",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4848c0d1-b8cc-401f-8ae8-c45f6dc402e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take our already prepared functions and classes\n",
    "def train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    prediction = model(batch_of_x.to(model.device)).transpose(1,2)\n",
    "    loss = loss_function(prediction.to(model.device), batch_of_y.to(model.device))\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.cpu().item()\n",
    "\n",
    "def train_epoch(train_generator, model, loss_function, optimizer, callback=None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        local_loss = train_on_batch( model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "        train_generator.set_postfix({'train batch loss': local_loss})\n",
    "\n",
    "        if callback is not None:\n",
    "            callback(model, local_loss)\n",
    "\n",
    "        epoch_loss += local_loss * len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "    \n",
    "    return epoch_loss/total\n",
    "\n",
    "def trainer(count_of_epoch, batch_size, model,\n",
    "            dataset, tag2idx, token2idx,\n",
    "            loss_function, optimizer, callback):\n",
    "    \n",
    "    iterations = tqdm(range(count_of_epoch))\n",
    "    for it in iterations:\n",
    "        optima = optimizer\n",
    "\n",
    "        number_of_batch = len(dataset[0]) // batch_size + (len(dataset[0])%batch_size > 0)\n",
    "        batch_generator = tqdm(dl_train)\n",
    "        \n",
    "        epoch_loss = train_epoch( train_generator = batch_generator, \n",
    "                                  model = model, \n",
    "                                  loss_function = loss_function, \n",
    "                                  optimizer = optima,\n",
    "                                  callback = callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})\n",
    "    \n",
    "class callback():\n",
    "    def __init__(self, writer, dataset, tag2idx, token2idx, loss_function, delimeter = 100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.tag2idx = tag2idx\n",
    "        self.token2idx = token2idx\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "        \n",
    "        if self.step % self.delimeter == 1:\n",
    "            \n",
    "            batch_generator = dl_test\n",
    "            \n",
    "            pred = []\n",
    "            real = []\n",
    "            \n",
    "            test_loss = 0\n",
    "            model.eval()\n",
    "            for it, (batch_of_x, batch_of_y) in enumerate(batch_generator):\n",
    "                batch_of_x = batch_of_x.to(model.device)\n",
    "                batch_of_y = batch_of_y.to(model.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    out = model(batch_of_x.to(model.device))\n",
    "                    test_loss += self.loss_function(out.transpose(1,2), batch_of_y).cpu().item()*len(batch_of_x)\n",
    "\n",
    "                pred.extend(torch.argmax(out, dim=-1).cpu().numpy().tolist())\n",
    "                real.extend(batch_of_y.cpu().numpy().tolist())\n",
    "\n",
    "            test_loss /= len(self.dataset[0])\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "            \n",
    "            ans=[]\n",
    "            my_ans=[]\n",
    "            for (y, my) in zip(real, pred):\n",
    "                for (i, idx) in enumerate(y):\n",
    "                    if idx != tag_to_id['[PAD]']:\n",
    "                        ans.append(id_to_tag[idx])\n",
    "                        my_ans.append(id_to_tag[my[i]])\n",
    "\n",
    "            self.writer.add_text('classification_report/test', str(classification_report(ans, my_ans)), self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)\n",
    "    \n",
    "class RNN(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "        \n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 output_dim,\n",
    "                 emb_dim = 20,\n",
    "                 hidden_dim = 20, \n",
    "                 num_layers = 3,\n",
    "                 dropout = 0,\n",
    "                 bnorm = False,\n",
    "                 bidirectional = False):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.num_direction = int(bidirectional + 1)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bnorm = bnorm\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "\n",
    "        self.encoder = torch.nn.LSTM(emb_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = self.embedding(input) \n",
    "        out, (h, c) = self.encoder(out)\n",
    "        if self.bnorm:\n",
    "            out = self.batchnorm(torch.transpose(out, 1, 2))\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebc221",
   "metadata": {},
   "source": [
    "### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2bef48-90a5-4f7b-a811-596d6fb96138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(2000002, 100)\n",
       "  (encoder): LSTM(100, 100, batch_first=True)\n",
       "  (classifier): Linear(in_features=100, out_features=19, bias=True)\n",
       "  (batchnorm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(**{\n",
    "    'bnorm': True,\n",
    "    'num_layers': 1,\n",
    "    'emb_dim' : 100,\n",
    "    'output_dim': len(unique_tags),\n",
    "    'vocab_dim': len(word_to_index),\n",
    "    'hidden_dim': 100,\n",
    "    'bidirectional': False}\n",
    ")\n",
    "\n",
    "# put model to cuda cores (or cpu)\n",
    "_ = model.to(device)\n",
    "model.embedding.weight.data.copy_(torch.tensor(fst_arr))\n",
    "\n",
    "# no grad for params\n",
    "for param in model.embedding.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# put real model to cuda\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771c881",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe75ff",
   "metadata": {},
   "source": [
    "Let's write `TestModel` for probing the model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b4288d-5617-488d-8512-2f2696511872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestModel(model, generator, tag_to_id, id_to_tag):\n",
    "    predictions, ys = list(), list()\n",
    "    model.eval()\n",
    "    \n",
    "    for _, batch in enumerate(generator):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].cpu().numpy().tolist()\n",
    "        with torch.no_grad():\n",
    "            prediction = torch.argmax(model(x_batch), dim=-1)\n",
    "        out = prediction.cpu().numpy().tolist()\n",
    "        predictions.extend(out)\n",
    "        ys.extend(y_batch)\n",
    "\n",
    "    result=[]\n",
    "    expected_result=[]\n",
    "    \n",
    "    for (y, prediction) in zip(ys, predictions):\n",
    "        for (id, idx) in enumerate(y):\n",
    "            if idx != tag_to_id['[PAD]']:\n",
    "                result.append(id_to_tag[idx])\n",
    "                expected_result.append(id_to_tag[prediction[id]])\n",
    "    \n",
    "    print(classification_report(result, expected_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc84d48-d53a-424d-8c06-fed70173210e",
   "metadata": {},
   "source": [
    "### Probe before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eae6186-0fe1-4b0b-8dbe-d97649b34b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.00      0.00      0.00     66919\n",
      "         ADP       0.00      0.00      0.00     84882\n",
      "         ADV       0.03      0.61      0.05     19495\n",
      "         AUX       0.00      0.00      0.00      4896\n",
      "       CCONJ       0.00      0.01      0.00     17486\n",
      "         DET       0.01      0.00      0.00     11409\n",
      "        INTJ       0.00      0.00      0.00        29\n",
      "        NOUN       0.06      0.00      0.00    203611\n",
      "         NUM       0.00      0.00      0.00     14558\n",
      "        PART       0.00      0.00      0.00      8957\n",
      "        PRON       0.02      0.15      0.04     26351\n",
      "       PROPN       0.00      0.00      0.00     53279\n",
      "       PUNCT       0.00      0.00      0.00    130097\n",
      "       SCONJ       0.00      0.00      0.00     12357\n",
      "         SYM       0.00      0.00      0.00       191\n",
      "        VERB       0.03      0.00      0.00     85192\n",
      "           X       0.00      0.00      0.00      7255\n",
      "\n",
      "    accuracy                           0.02    746964\n",
      "   macro avg       0.01      0.05      0.01    746964\n",
      "weighted avg       0.02      0.02      0.00    746964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TestModel(model, dl_train, tag_to_id, id_to_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc8bdb-ee63-4d28-853e-477c480b2982",
   "metadata": {},
   "source": [
    "### Training process\n",
    "Let's train model with `AdamW` and `CEL` loss function. Note how we are ignoring IDs of `'[PAD]'` since those shouldn't be in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c916422-57e5-4530-a97e-58622f8ee97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc28996fb2b4a26818c1f6519f26834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41feb41b646b4639afe1865f6356ec91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4c0f99cca542dcacb461f3331f57a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7798cd3eddf4126a869b652376e4b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2ee34a6c0f4932a9b9ab6b4372eef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(ignore_index=tag_to_id['[PAD]'])\n",
    "# for all parameters no grad is required\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# setup writer\n",
    "writer = SummaryWriter(log_dir = f'./log')\n",
    "\n",
    "\n",
    "# let's try training\n",
    "cb_result = callback(writer, d_test, tag_to_id, word_to_index, loss, delimeter = 150)\n",
    "trainer(count_of_epoch=4, batch_size=64,\n",
    "        dataset=d_train, model=model,\n",
    "        tag2idx=tag_to_id, token2idx=word_to_index,\n",
    "        loss_function=loss, optimizer=opt,\n",
    "        callback=cb_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc69f48-d0bc-4f71-93e9-ddd964e18187",
   "metadata": {},
   "source": [
    "### Probe after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f128651-bb9a-418c-bf6e-66556b98e600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.93      0.93     66919\n",
      "         ADP       1.00      1.00      1.00     84882\n",
      "         ADV       0.94      0.89      0.92     19495\n",
      "         AUX       0.93      0.99      0.96      4896\n",
      "       CCONJ       0.98      0.99      0.98     17486\n",
      "         DET       0.86      0.85      0.86     11409\n",
      "        INTJ       0.58      0.24      0.34        29\n",
      "        NOUN       0.98      0.98      0.98    203611\n",
      "         NUM       0.94      0.93      0.94     14558\n",
      "        PART       0.97      0.90      0.93      8957\n",
      "        PRON       0.94      0.94      0.94     26351\n",
      "       PROPN       0.92      0.95      0.93     53279\n",
      "       PUNCT       1.00      1.00      1.00    130097\n",
      "       SCONJ       0.93      0.98      0.96     12357\n",
      "         SYM       0.66      0.87      0.75       191\n",
      "        VERB       0.97      0.96      0.97     85192\n",
      "           X       0.88      0.82      0.85      7255\n",
      "\n",
      "    accuracy                           0.97    746964\n",
      "   macro avg       0.91      0.90      0.90    746964\n",
      "weighted avg       0.97      0.97      0.97    746964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TestModel(model, dl_test, tag_to_id, id_to_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49a97a-ffc8-4c99-9f38-e20746f08087",
   "metadata": {},
   "source": [
    "# Loading tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44906c66-fa86-4bc7-919d-65c7f93bce04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3585901), started 0:06:20 ago. (Use '!kill 3585901' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-19db08299774afa2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-19db08299774afa2\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir './log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc9c3b-beb7-44cd-9833-88e61836ee00",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "- `Fasttext` achieves incredible results even since first epoch.\n",
    "- Pre-trained model is better for results.\n",
    "- `reduce_model` is extremely heavy and requires more than 30gb of RAM in this case.\n",
    "- `Fasttext` is good if you can run in on cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
